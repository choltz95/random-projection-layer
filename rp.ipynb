{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rp.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2_S5KBEX2Al",
        "colab_type": "code",
        "outputId": "54a04fe3-7b0c-4fbe-9590-2b8d3ebf0533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!pip3 install cleverhans"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cleverhans in /usr/local/lib/python3.6/dist-packages (3.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.5.0)\n",
            "Requirement already satisfied: pycodestyle in /usr/local/lib/python3.6/dist-packages (from cleverhans) (2.4.0)\n",
            "Requirement already satisfied: mnist~=0.2 in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.2.2)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.3.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.14.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (1.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.3.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2018.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn7FV0GMXfq_",
        "colab_type": "code",
        "outputId": "d07066d4-5ee6-4e31-f774-24b631e218ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "This tutorial shows how to generate adversarial examples using FGSM\n",
        "and train a model using adversarial training with Keras.\n",
        "It is very similar to mnist_tutorial_tf.py, which does the same\n",
        "thing but without a dependence on keras.\n",
        "The original paper can be found at:\n",
        "https://arxiv.org/abs/1412.6572\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.platform import flags\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import backend\n",
        "\n",
        "from cleverhans.attacks import FastGradientMethod\n",
        "from cleverhans.dataset import MNIST\n",
        "from cleverhans.loss import CrossEntropy\n",
        "from cleverhans.train import train\n",
        "from cleverhans.utils import AccuracyReport\n",
        "#from cleverhans.utils_keras import cnn_model\n",
        "from cleverhans.utils_keras import KerasModelWrapper\n",
        "from cleverhans.utils_tf import model_eval\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "NB_EPOCHS = 10\n",
        "BATCH_SIZE = 256\n",
        "LEARNING_RATE = .001\n",
        "TRAIN_DIR = 'train_dir'\n",
        "FILENAME = 'mnist.ckpt'\n",
        "LOAD_MODEL = False"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PwJY0RqSzve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "with warnings.catch_warnings():\n",
        "  warnings.simplefilter('ignore', category=ImportWarning)\n",
        "  from sklearn import random_projection\n",
        "  from sklearn.decomposition import PCA\n",
        "\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense, Activation, Flatten, BatchNormalization, Lambda\n",
        "\n",
        "\n",
        "  \n",
        "def mlp_model(logits=False, input_ph=None, input_dim=28*28,\n",
        "              channels=1, nb_units=100, nb_classes=10):\n",
        "  \"\"\"\n",
        "  Defines a mlp model using Keras sequential model\n",
        "  :param logits: If set to False, returns a Keras model, otherwise will also\n",
        "                  return logits tensor\n",
        "  :param input_ph: The TensorFlow tensor for the input\n",
        "                  (needed if returning logits)\n",
        "                  (\"ph\" stands for placeholder but it need not actually be a\n",
        "                  placeholder)\n",
        "  :param img_rows: number of row in the image\n",
        "  :param img_cols: number of columns in the image\n",
        "  :param channels: number of color channels (e.g., 1 for MNIST)\n",
        "  :param nb_filters: number of convolutional filters per layer\n",
        "  :param nb_classes: the number of output classes\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  model = Sequential()\n",
        "\n",
        "  if keras.backend.image_dim_ordering() == 'th':\n",
        "    input_shape = (channels, input_dim)\n",
        "  else:\n",
        "    input_shape = (input_dim, channels)\n",
        "\n",
        "  layers = [Dense(units=nb_units, input_dim=input_dim),\n",
        "          BatchNormalization(),\n",
        "          Dense(units=nb_units),\n",
        "          Activation('relu'),\n",
        "          BatchNormalization(),\n",
        "          Dense(units=nb_units),\n",
        "          Activation('relu'),\n",
        "          BatchNormalization(),\n",
        "          Dense(nb_classes)]\n",
        "  \n",
        "  for layer in layers:\n",
        "    model.add(layer)\n",
        "\n",
        "  if logits:\n",
        "    logits_tensor = model(input_ph)\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  if logits:\n",
        "    return model, logits_tensor, model.trainable_weights\n",
        "  else:\n",
        "    return model, model.trainable_weights\n",
        "  \n",
        "initial_weights = None\n",
        "def rp(input_dim=28*28, projected_dim=400, channels=1, stop_grad='total', a='relu', name='rp_layer'):\n",
        "  def init(shape, dtype=tf.float32):\n",
        "    rp_transformer = random_projection.SparseRandomProjection(n_components=shape[1], dense_output=True, density=1/3)\n",
        "    projection = rp_transformer._make_random_matrix(shape[0], shape[1]).todense()\n",
        "    projection = tf.convert_to_tensor(projection, dtype=tf.float32)\n",
        "    if stop_grad == 'full':\n",
        "      projection = keras.backend.stop_gradient(projection)\n",
        "    if stop_grad == 'partial':\n",
        "      projection = tf.where(projection == 0, keras.backend.stop_gradient(projection), projection)\n",
        "    global initial_weights\n",
        "    initial_weights = projection\n",
        "    return projection\n",
        "  \n",
        "  projection_layer = Dense(units=projected_dim, input_dim=input_dim, kernel_initializer=init, use_bias=False, activation=a, trainable=False, name=name)\n",
        "  return projection_layer\n",
        "  \n",
        "def rp_mlp_model(logits=False, input_ph=None, input_dim=28*28,\n",
        "              channels=1, nb_units=100, nb_classes=10):\n",
        "  var_list = []\n",
        "  model = Sequential()\n",
        "\n",
        "  if keras.backend.image_dim_ordering() == 'th':\n",
        "    input_shape = (channels, input_dim)\n",
        "  else:\n",
        "    input_shape = (input_dim, channels)\n",
        "\n",
        "  #layers = [Dense(units=nb_units, input_dim=input_dim),\n",
        "  layers=[rp(name='rp_layer1',projected_dim=80),\n",
        "          Activation('relu'),\n",
        "          BatchNormalization(),\n",
        "          Dense(units=nb_units),\n",
        "          Activation('relu'),\n",
        "          BatchNormalization(),\n",
        "          rp(name='rp_layer2',projected_dim=80),\n",
        "          Activation('relu'),\n",
        "          BatchNormalization(),\n",
        "          Dense(units=nb_units),\n",
        "          Activation('relu'),\n",
        "          BatchNormalization(),\n",
        "          Dense(nb_classes)]\n",
        "  \n",
        "  for layer in layers:\n",
        "    model.add(layer)\n",
        "  \n",
        "  tvars = model.trainable_weights\n",
        "  var_list = [var for var in tvars if 'rp_layer' not in var.name]\n",
        "\n",
        "  if logits:\n",
        "    logits_tensor = model(input_ph)\n",
        "  model.add(Activation('softmax'))\n",
        "  \n",
        "  if logits:\n",
        "    return model, logits_tensor, var_list\n",
        "  else:\n",
        "    return model, var_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYKttxSVXqU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mnist(train_start=0, train_end=60000, test_start=0,\n",
        "                   test_end=10000, nb_epochs=NB_EPOCHS, batch_size=BATCH_SIZE,\n",
        "                   learning_rate=LEARNING_RATE, train_dir=TRAIN_DIR,\n",
        "                   filename=FILENAME, load_model=LOAD_MODEL,\n",
        "                   testing=False, label_smoothing=0.1):\n",
        "  \"\"\"\n",
        "  MNIST CleverHans tutorial\n",
        "  :param train_start: index of first training set example\n",
        "  :param train_end: index of last training set example\n",
        "  :param test_start: index of first test set example\n",
        "  :param test_end: index of last test set example\n",
        "  :param nb_epochs: number of epochs to train model\n",
        "  :param batch_size: size of training batches\n",
        "  :param learning_rate: learning rate for training\n",
        "  :param train_dir: Directory storing the saved model\n",
        "  :param filename: Filename to save model under\n",
        "  :param load_model: True for load, False for not load\n",
        "  :param testing: if true, test error is calculated\n",
        "  :param label_smoothing: float, amount of label smoothing for cross entropy\n",
        "  :return: an AccuracyReport object\n",
        "  \"\"\"\n",
        "  keras.layers.core.K.set_learning_phase(0)\n",
        "\n",
        "  # Object used to keep track of (and return) key accuracies\n",
        "  report = AccuracyReport()\n",
        "\n",
        "  # Set TF random seed to improve reproducibility\n",
        "  tf.set_random_seed(1234)\n",
        "\n",
        "  if not hasattr(backend, \"tf\"):\n",
        "    raise RuntimeError(\"This tutorial requires keras to be configured\"\n",
        "                       \" to use the TensorFlow backend.\")\n",
        "\n",
        "  if keras.backend.image_dim_ordering() != 'tf':\n",
        "    keras.backend.set_image_dim_ordering('tf')\n",
        "    print(\"INFO: '~/.keras/keras.json' sets 'image_dim_ordering' to \"\n",
        "          \"'th', temporarily setting to 'tf'\")\n",
        "\n",
        "  # Create TF session and set as Keras backend session\n",
        "  sess = tf.Session()\n",
        "  keras.backend.set_session(sess)\n",
        "\n",
        "  # Get MNIST test data\n",
        "  mnist = MNIST(train_start=train_start, train_end=train_end,\n",
        "                test_start=test_start, test_end=test_end)\n",
        "  x_train, y_train = mnist.get_set('train')\n",
        "  x_test, y_test = mnist.get_set('test')\n",
        "  \n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "\n",
        "  # Obtain Image Parameters\n",
        "  img_rows, img_cols, nchannels = x_train.shape[1:4]\n",
        "  nb_classes = y_train.shape[1]\n",
        "\n",
        "  # Define input TF placeholder\n",
        "  x = tf.placeholder(tf.float32, shape=(None, img_rows, img_cols,\n",
        "                                        nchannels))\n",
        "  y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
        "  \n",
        "  x_train = x_train.reshape((-1,28*28))\n",
        "  x_test = x_test.reshape((-1,28*28))\n",
        "  x = tf.reshape(x,(-1,28*28))\n",
        "\n",
        "  # Define TF model graph\n",
        "  #model = cnn_model(img_rows=img_rows, img_cols=img_cols,\n",
        "  #                  channels=nchannels, nb_filters=64,\n",
        "  #                  nb_classes=nb_classes)\n",
        "  \"\"\"\n",
        "  model = mlp_model(input_dim=img_rows*img_cols,\n",
        "                    channels=nchannels, nb_units=64,\n",
        "                    nb_classes=nb_classes)\n",
        "  preds = model(x)\n",
        "  \n",
        "  print(\"Defined TensorFlow model graph.\")\n",
        "\n",
        "  def evaluate():\n",
        "    # Evaluate the accuracy of the MNIST model on legitimate test examples\n",
        "    eval_params = {'batch_size': batch_size}\n",
        "    acc = model_eval(sess, x, y, preds, x_test, y_test, args=eval_params)\n",
        "    report.clean_train_clean_eval = acc\n",
        "    print('Test accuracy on legitimate examples: %0.4f' % acc)\n",
        "\n",
        "  # Train an MNIST model\n",
        "  train_params = {\n",
        "      'nb_epochs': nb_epochs,\n",
        "      'batch_size': batch_size,\n",
        "      'learning_rate': learning_rate,\n",
        "      'train_dir': train_dir,\n",
        "      'filename': filename\n",
        "  }\n",
        "\n",
        "  rng = np.random.RandomState([2017, 8, 30])\n",
        "  if not os.path.exists(train_dir):\n",
        "    os.mkdir(train_dir)\n",
        "\n",
        "  ckpt = tf.train.get_checkpoint_state(train_dir)\n",
        "  print(train_dir, ckpt)\n",
        "  ckpt_path = False if ckpt is None else ckpt.model_checkpoint_path\n",
        "  wrap = KerasModelWrapper(model)\n",
        "\n",
        "  if load_model and ckpt_path:\n",
        "    saver = tf.train.Saver()\n",
        "    print(ckpt_path)\n",
        "    saver.restore(sess, ckpt_path)\n",
        "    print(\"Model loaded from: {}\".format(ckpt_path))\n",
        "    evaluate()\n",
        "  else:\n",
        "    print(\"Model was not loaded, training from scratch.\")\n",
        "    loss = CrossEntropy(wrap, smoothing=label_smoothing)\n",
        "    train(sess, loss, x_train, y_train, evaluate=evaluate,\n",
        "          args=train_params, rng=rng)\n",
        "\n",
        "  # Calculate training error\n",
        "  if testing:\n",
        "    eval_params = {'batch_size': batch_size}\n",
        "    acc = model_eval(sess, x, y, preds, x_train, y_train, args=eval_params)\n",
        "    report.train_clean_train_clean_eval = acc\n",
        "\n",
        "  # Initialize the Fast Gradient Sign Method (FGSM) attack object and graph\n",
        "  fgsm = FastGradientMethod(wrap, sess=sess)\n",
        "  fgsm_params = {'eps': 0.3,\n",
        "                 'clip_min': 0.,\n",
        "                 'clip_max': 1.}\n",
        "  adv_x = fgsm.generate(x, **fgsm_params)\n",
        "  # Consider the attack to be constant\n",
        "  adv_x = tf.stop_gradient(adv_x)\n",
        "  preds_adv = model(adv_x)\n",
        "\n",
        "  # Evaluate the accuracy of the MNIST model on adversarial examples\n",
        "  eval_par = {'batch_size': batch_size}\n",
        "  acc = model_eval(sess, x, y, preds_adv, x_test, y_test, args=eval_par)\n",
        "  print('Test accuracy on adversarial examples: %0.4f\\n' % acc)\n",
        "  report.clean_train_adv_eval = acc\n",
        "\n",
        "  # Calculating train error\n",
        "  if testing:\n",
        "    eval_par = {'batch_size': batch_size}\n",
        "    acc = model_eval(sess, x, y, preds_adv, x_train,\n",
        "                     y_train, args=eval_par)\n",
        "    report.train_clean_train_adv_eval = acc\n",
        "\n",
        "  \"\"\"\n",
        "  # RP augmented mlp\n",
        "  ######################################\n",
        "  \n",
        "  model, var_list = rp_mlp_model(input_dim=img_rows*img_cols,\n",
        "                      channels=nchannels, nb_units=100,\n",
        "                      nb_classes=nb_classes)\n",
        "\n",
        "  preds = model(x)\n",
        "  \n",
        "  print(\"Defined TensorFlow model graph.\")\n",
        "\n",
        "  def evaluate():\n",
        "    # Evaluate the accuracy of the MNIST model on legitimate test examples\n",
        "    eval_params = {'batch_size': batch_size}\n",
        "    acc = model_eval(sess, x, y, preds, x_test, y_test, args=eval_params)\n",
        "    report.clean_train_clean_eval = acc\n",
        "    print('Test accuracy on legitimate examples: %0.4f' % acc)\n",
        "\n",
        "  # Train an MNIST model\n",
        "  train_params = {\n",
        "      'nb_epochs': nb_epochs,\n",
        "      'batch_size': batch_size,\n",
        "      'learning_rate': learning_rate,\n",
        "      'train_dir': train_dir,\n",
        "      'filename': filename\n",
        "  }\n",
        "\n",
        "  rng = np.random.RandomState([2017, 8, 30])\n",
        "  if not os.path.exists(train_dir):\n",
        "    os.mkdir(train_dir)\n",
        "\n",
        "  ckpt = tf.train.get_checkpoint_state(train_dir)\n",
        "  print(train_dir, ckpt)\n",
        "  ckpt_path = False if ckpt is None else ckpt.model_checkpoint_path\n",
        "  wrap = KerasModelWrapper(model)\n",
        "\n",
        "  if load_model and ckpt_path:\n",
        "    saver = tf.train.Saver()\n",
        "    print(ckpt_path)\n",
        "    saver.restore(sess, ckpt_path)\n",
        "    print(\"Model loaded from: {}\".format(ckpt_path))\n",
        "    evaluate()\n",
        "  else:\n",
        "    print(\"Model was not loaded, training from scratch.\")\n",
        "    loss = CrossEntropy(wrap, smoothing=label_smoothing)\n",
        "    train(sess, loss, x_train, y_train, var_list=var_list, evaluate=evaluate,\n",
        "          args=train_params, rng=rng)\n",
        "\n",
        "  # Calculate training error\n",
        "  if testing:\n",
        "    eval_params = {'batch_size': batch_size}\n",
        "    acc = model_eval(sess, x, y, preds, x_train, y_train, args=eval_params)\n",
        "    report.train_clean_train_clean_eval = acc\n",
        "\n",
        "  # Initialize the Fast Gradient Sign Method (FGSM) attack object and graph\n",
        "  fgsm = FastGradientMethod(wrap, sess=sess)\n",
        "  fgsm_params = {'eps': 0.01,\n",
        "                 'clip_min': 0.,\n",
        "                 'clip_max': 1.}\n",
        "  adv_x = fgsm.generate(x, **fgsm_params)\n",
        "  # Consider the attack to be constant\n",
        "  adv_x = tf.stop_gradient(adv_x)\n",
        "  preds_adv = model(adv_x)\n",
        "\n",
        "  # Evaluate the accuracy of the MNIST model on adversarial examples\n",
        "  eval_par = {'batch_size': batch_size}\n",
        "  acc = model_eval(sess, x, y, preds_adv, x_test, y_test, args=eval_par)\n",
        "  print('Test accuracy on adversarial examples: %0.4f\\n' % acc)\n",
        "  report.clean_train_adv_eval = acc\n",
        "\n",
        "  # Calculating train error\n",
        "  if testing:\n",
        "    eval_par = {'batch_size': batch_size}\n",
        "    acc = model_eval(sess, x, y, preds_adv, x_train,\n",
        "                     y_train, args=eval_par)\n",
        "    report.train_clean_train_adv_eval = acc\n",
        "        \n",
        "  global m \n",
        "  m = model\n",
        "  return report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ewRgIYdXuNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = None\n",
        "def main(argv=None):\n",
        "  #from cleverhans_tutorials import check_installation\n",
        "  #check_installation(__file__)\n",
        "  for i in range(0,5):\n",
        "    report = mnist_tutorial(nb_epochs=NB_EPOCHS,\n",
        "                       batch_size=BATCH_SIZE,\n",
        "                       learning_rate=LEARNING_RATE,\n",
        "                       train_dir=TRAIN_DIR,\n",
        "                       filename=FILENAME,\n",
        "                       load_model=LOAD_MODEL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk9W5slSXwXp",
        "colab_type": "code",
        "outputId": "3c86b6cc-8243-4c54-ebc2-ea1b2c06f003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2227
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  \"\"\"\n",
        "  flags.DEFINE_integer('nb_epochs', NB_EPOCHS,\n",
        "                       'Number of epochs to train model')\n",
        "  flags.DEFINE_integer('batch_size', BATCH_SIZE, 'Size of training batches')\n",
        "  flags.DEFINE_float('learning_rate', LEARNING_RATE,\n",
        "                     'Learning rate for training')\n",
        "  flags.DEFINE_string('train_dir', TRAIN_DIR,\n",
        "                      'Directory where to save model.')\n",
        "  flags.DEFINE_string('filename', FILENAME, 'Checkpoint filename.')\n",
        "  flags.DEFINE_boolean('load_model', LOAD_MODEL,\n",
        "                       'Load saved model or train.')\n",
        "  \"\"\"\n",
        "  main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defined TensorFlow model graph.\n",
            "train_dir None\n",
            "Model was not loaded, training from scratch.\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:36:50,754 cleverhans] Epoch 0 took 1.1731157302856445 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.1135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:36:51,913 cleverhans] Epoch 1 took 0.9326636791229248 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.3624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:36:53,013 cleverhans] Epoch 2 took 0.9309670925140381 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5041\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:36:54,111 cleverhans] Epoch 3 took 0.9289872646331787 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:36:55,206 cleverhans] Epoch 4 took 0.9258706569671631 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6318\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:36:56,306 cleverhans] Epoch 5 took 0.9305362701416016 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:36:57,421 cleverhans] Epoch 6 took 0.945347785949707 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:36:58,523 cleverhans] Epoch 7 took 0.933025598526001 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.7140\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:36:59,620 cleverhans] Epoch 8 took 0.9262049198150635 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.7321\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:00,719 cleverhans] Epoch 9 took 0.9285850524902344 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.7548\n",
            "Test accuracy on adversarial examples: 0.0401\n",
            "\n",
            "Defined TensorFlow model graph.\n",
            "train_dir None\n",
            "Model was not loaded, training from scratch.\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:06,392 cleverhans] Epoch 0 took 1.0640618801116943 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.1135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:07,610 cleverhans] Epoch 1 took 0.9298207759857178 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.3636\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:08,718 cleverhans] Epoch 2 took 0.936478853225708 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.3761\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:09,828 cleverhans] Epoch 3 took 0.9407827854156494 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.4598\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:10,933 cleverhans] Epoch 4 took 0.9363627433776855 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:12,038 cleverhans] Epoch 5 took 0.932762861251831 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:13,142 cleverhans] Epoch 6 took 0.934406042098999 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:14,246 cleverhans] Epoch 7 took 0.9323530197143555 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6405\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:15,352 cleverhans] Epoch 8 took 0.9360742568969727 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6554\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:16,454 cleverhans] Epoch 9 took 0.9267375469207764 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6986\n",
            "Test accuracy on adversarial examples: 0.0256\n",
            "\n",
            "Defined TensorFlow model graph.\n",
            "train_dir None\n",
            "Model was not loaded, training from scratch.\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:22,747 cleverhans] Epoch 0 took 1.098975658416748 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.0980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:24,068 cleverhans] Epoch 1 took 0.9442038536071777 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.4481\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:25,186 cleverhans] Epoch 2 took 0.9453246593475342 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6112\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:26,298 cleverhans] Epoch 3 took 0.9419639110565186 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6253\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:27,404 cleverhans] Epoch 4 took 0.9373288154602051 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:28,514 cleverhans] Epoch 5 took 0.9396772384643555 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.7155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:29,624 cleverhans] Epoch 6 took 0.9376223087310791 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6650\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:30,734 cleverhans] Epoch 7 took 0.939629316329956 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.7431\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:31,847 cleverhans] Epoch 8 took 0.9398870468139648 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.7315\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:33,006 cleverhans] Epoch 9 took 0.9725732803344727 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.7711\n",
            "Test accuracy on adversarial examples: 0.0304\n",
            "\n",
            "Defined TensorFlow model graph.\n",
            "train_dir None\n",
            "Model was not loaded, training from scratch.\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:40,051 cleverhans] Epoch 0 took 1.140186071395874 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.1135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:41,610 cleverhans] Epoch 1 took 0.9638721942901611 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.3025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:42,727 cleverhans] Epoch 2 took 0.9436612129211426 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.4352\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:43,844 cleverhans] Epoch 3 took 0.9452259540557861 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5186\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:44,951 cleverhans] Epoch 4 took 0.9372632503509521 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6038\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:46,060 cleverhans] Epoch 5 took 0.9355363845825195 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:47,164 cleverhans] Epoch 6 took 0.9325413703918457 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:48,280 cleverhans] Epoch 7 took 0.9451084136962891 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6842\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:49,382 cleverhans] Epoch 8 took 0.9215095043182373 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.7066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:50,505 cleverhans] Epoch 9 took 0.9392745494842529 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.7381\n",
            "Test accuracy on adversarial examples: 0.0298\n",
            "\n",
            "Defined TensorFlow model graph.\n",
            "train_dir None\n",
            "Model was not loaded, training from scratch.\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:37:58,501 cleverhans] Epoch 0 took 1.1628005504608154 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.1626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:38:00,120 cleverhans] Epoch 1 took 0.9359831809997559 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.3576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:38:01,230 cleverhans] Epoch 2 took 0.9360814094543457 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5370\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:38:02,337 cleverhans] Epoch 3 took 0.939488410949707 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:38:03,450 cleverhans] Epoch 4 took 0.9388422966003418 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6730\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:38:04,559 cleverhans] Epoch 5 took 0.9363813400268555 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.7072\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:38:05,671 cleverhans] Epoch 6 took 0.9393424987792969 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.7419\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:38:06,781 cleverhans] Epoch 7 took 0.9335837364196777 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.7501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:38:07,890 cleverhans] Epoch 8 took 0.9296507835388184 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.7809\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2018-12-31 20:38:09,006 cleverhans] Epoch 9 took 0.9398238658905029 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.7908\n",
            "Test accuracy on adversarial examples: 0.0373\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbDEsHe5qnD1",
        "colab_type": "code",
        "outputId": "0fd6dfc9-e222-47e2-9f0a-2c0937efec66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "Defined TensorFlow model graph.\n",
        "train_dir None\n",
        "Model was not loaded, training from scratch.\n",
        "num_devices:  1\n",
        "[INFO 2018-12-31 19:15:26,702 cleverhans] Epoch 0 took 1.125884771347046 seconds\n",
        "Test accuracy on legitimate examples: 0.7984\n",
        "[INFO 2018-12-31 19:15:27,795 cleverhans] Epoch 1 took 0.8888399600982666 seconds\n",
        "Test accuracy on legitimate examples: 0.8848\n",
        "[INFO 2018-12-31 19:15:28,855 cleverhans] Epoch 2 took 0.8946611881256104 seconds\n",
        "Test accuracy on legitimate examples: 0.9120\n",
        "[INFO 2018-12-31 19:15:29,910 cleverhans] Epoch 3 took 0.8911709785461426 seconds\n",
        "Test accuracy on legitimate examples: 0.9177\n",
        "[INFO 2018-12-31 19:15:30,964 cleverhans] Epoch 4 took 0.8903017044067383 seconds\n",
        "Test accuracy on legitimate examples: 0.9256\n",
        "[INFO 2018-12-31 19:15:32,022 cleverhans] Epoch 5 took 0.8943595886230469 seconds\n",
        "Test accuracy on legitimate examples: 0.9277\n",
        "[INFO 2018-12-31 19:15:33,074 cleverhans] Epoch 6 took 0.888146162033081 seconds\n",
        "Test accuracy on legitimate examples: 0.9337\n",
        "[INFO 2018-12-31 19:15:34,125 cleverhans] Epoch 7 took 0.8888247013092041 seconds\n",
        "Test accuracy on legitimate examples: 0.9362\n",
        "[INFO 2018-12-31 19:15:35,177 cleverhans] Epoch 8 took 0.8878788948059082 seconds\n",
        "Test accuracy on legitimate examples: 0.9404\n",
        "[INFO 2018-12-31 19:15:36,233 cleverhans] Epoch 9 took 0.8925678730010986 seconds\n",
        "Test accuracy on legitimate examples: 0.9424\n",
        "Test accuracy on adversarial examples: 0.0662\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDefined TensorFlow model graph.\\ntrain_dir None\\nModel was not loaded, training from scratch.\\nnum_devices:  1\\n[INFO 2018-12-31 19:15:26,702 cleverhans] Epoch 0 took 1.125884771347046 seconds\\nTest accuracy on legitimate examples: 0.7984\\n[INFO 2018-12-31 19:15:27,795 cleverhans] Epoch 1 took 0.8888399600982666 seconds\\nTest accuracy on legitimate examples: 0.8848\\n[INFO 2018-12-31 19:15:28,855 cleverhans] Epoch 2 took 0.8946611881256104 seconds\\nTest accuracy on legitimate examples: 0.9120\\n[INFO 2018-12-31 19:15:29,910 cleverhans] Epoch 3 took 0.8911709785461426 seconds\\nTest accuracy on legitimate examples: 0.9177\\n[INFO 2018-12-31 19:15:30,964 cleverhans] Epoch 4 took 0.8903017044067383 seconds\\nTest accuracy on legitimate examples: 0.9256\\n[INFO 2018-12-31 19:15:32,022 cleverhans] Epoch 5 took 0.8943595886230469 seconds\\nTest accuracy on legitimate examples: 0.9277\\n[INFO 2018-12-31 19:15:33,074 cleverhans] Epoch 6 took 0.888146162033081 seconds\\nTest accuracy on legitimate examples: 0.9337\\n[INFO 2018-12-31 19:15:34,125 cleverhans] Epoch 7 took 0.8888247013092041 seconds\\nTest accuracy on legitimate examples: 0.9362\\n[INFO 2018-12-31 19:15:35,177 cleverhans] Epoch 8 took 0.8878788948059082 seconds\\nTest accuracy on legitimate examples: 0.9404\\n[INFO 2018-12-31 19:15:36,233 cleverhans] Epoch 9 took 0.8925678730010986 seconds\\nTest accuracy on legitimate examples: 0.9424\\nTest accuracy on adversarial examples: 0.0662\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEPbJpT4TJIZ",
        "colab_type": "code",
        "outputId": "753f564f-e8a4-46c5-ef15-a064c6dfdb49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "m.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rp_layer1 (Dense)            (None, 80)                62720     \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 80)                320       \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               8100      \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "rp_layer2 (Dense)            (None, 80)                8000      \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 80)                320       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 100)               8100      \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 89,370\n",
            "Trainable params: 17,930\n",
            "Non-trainable params: 71,440\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_imHyFt7M2Br",
        "colab_type": "code",
        "outputId": "c72bca2d-ac87-4dc5-823b-c9f720478a00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "# ensure rp layers not training\n",
        "initial_weights = tf.Session().run(initial_weights)\n",
        "final_weights = m.layers[0].get_weights()[0]\n",
        "np.allclose(final_weights, initial_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b49110f8156a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minitial_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfinal_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mallclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2255\u001b[0m     \"\"\"\n\u001b[0;32m-> 2256\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequal_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mequal_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2257\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36misclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2333\u001b[0m     \u001b[0myfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2335\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwithin_tol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2336\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2337\u001b[0m         \u001b[0mfinite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxfin\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0myfin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mwithin_tol\u001b[0;34m(x, y, atol, rtol)\u001b[0m\n\u001b[1;32m   2319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithin_tol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2320\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2321\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mless_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrtol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2323\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (784,80) (100,80) "
          ]
        }
      ]
    }
  ]
}